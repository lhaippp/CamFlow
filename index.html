<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>[ICCV25] Estimating 2D Camera Motion with Hybrid Motion Basis</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="styles.css">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #fafafa;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }

        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 60px 0;
            text-align: center;
        }

        .conference-tag {
            background: rgba(255, 255, 255, 0.2);
            display: inline-block;
            padding: 8px 16px;
            border-radius: 20px;
            font-size: 14px;
            font-weight: 500;
            margin-bottom: 20px;
            backdrop-filter: blur(10px);
        }

        h1 {
            font-size: 2.5rem;
            font-weight: 700;
            margin-bottom: 30px;
            line-height: 1.2;
        }

        .authors {
            font-size: 1.1rem;
            margin-bottom: 20px;
            opacity: 0.9;
        }

        .affiliation {
            font-size: 1rem;
            opacity: 0.8;
        }

        .nav-buttons {
            margin-top: 40px;
            display: flex;
            justify-content: center;
            gap: 20px;
            flex-wrap: wrap;
        }

        .btn {
            display: inline-block;
            padding: 12px 24px;
            background: rgba(255, 255, 255, 0.2);
            color: white;
            text-decoration: none;
            border-radius: 25px;
            font-weight: 500;
            transition: all 0.3s ease;
            backdrop-filter: blur(10px);
            border: 1px solid rgba(255, 255, 255, 0.3);
        }

        .btn:hover {
            background: rgba(255, 255, 255, 0.3);
            transform: translateY(-2px);
        }

        main {
            padding: 60px 0;
        }

        section {
            margin-bottom: 60px;
        }

        h2 {
            font-size: 2rem;
            font-weight: 600;
            margin-bottom: 30px;
            color: #2c3e50;
            text-align: center;
        }

        .abstract-box {
            background: white;
            padding: 40px;
            border-radius: 15px;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.1);
            margin-bottom: 40px;
        }

        .abstract-box p {
            font-size: 1.1rem;
            line-height: 1.8;
            text-align: justify;
        }

        .method-overview {
            background: white;
            padding: 40px;
            border-radius: 15px;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.1);
            text-align: center;
        }

        .method-diagram {
            width: 100%;
            max-width: 800px;
            height: 400px;
            background: linear-gradient(45deg, #f0f2f5, #e1e8ed);
            border-radius: 10px;
            margin: 30px auto;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.2rem;
            color: #666;
            border: 2px dashed #ccc;
        }

        .results-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 30px;
            margin-top: 40px;
        }

        .result-card {
            background: white;
            padding: 30px;
            border-radius: 15px;
            box-shadow: 0 5px 20px rgba(0, 0, 0, 0.08);
            text-align: center;
        }

        .result-card h3 {
            font-size: 1.3rem;
            margin-bottom: 15px;
            color: #667eea;
        }

        .result-placeholder {
            width: 100%;
            height: 200px;
            background: linear-gradient(45deg, #f8f9fa, #e9ecef);
            border-radius: 8px;
            margin: 20px 0;
            display: flex;
            align-items: center;
            justify-content: center;
            color: #6c757d;
            border: 1px dashed #dee2e6;
        }

        .citation-box {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 10px;
            border-left: 4px solid #667eea;
            font-family: 'Courier New', monospace;
            font-size: 0.9rem;
            overflow-x: auto;
        }

        footer {
            background: #2c3e50;
            color: white;
            text-align: center;
            padding: 40px 0;
        }

        .social-links {
            margin-top: 20px;
        }

        .social-links a {
            color: white;
            text-decoration: none;
            margin: 0 10px;
            font-size: 1.1rem;
        }

        .social-links a:hover {
            color: #667eea;
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 2rem;
            }
            
            .nav-buttons {
                flex-direction: column;
                align-items: center;
            }
            
            .btn {
                width: 200px;
            }
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <div class="conference-info">
                <img src="images/iccv2025-logo.png" alt="ICCV 2025" class="conference-logo">
            </div>
            <h1>Estimating 2D Camera Motion with Hybrid Motion Basis</h1>
            <div class="authors">
                <strong>Haipeng Li</strong><sup>1*</sup>, 
                <strong>Tianhao Zhou</strong><sup>1*</sup>, 
                Zhanglei Yang<sup>1</sup>, 
                Yi Wu<sup>2</sup>, 
                Yan Chen<sup>2</sup>, 
                Zijing Mao<sup>2</sup>, 
                Shen Cheng<sup>3</sup>, 
                Bing Zeng<sup>1</sup>, 
                Shuaicheng Liu<sup>1â€ </sup>
            </div>
            <div class="affiliation">
                <sup>1</sup>University of Electronic Science and Technology of China, 
                <sup>2</sup>Xiaomi Corporation, 
                <sup>3</sup>Dexmal
                <br>
                <small><sup>*</sup>Equal contribution, <sup>â€ </sup>Corresponding author</small>
            </div>
            <div class="nav-buttons">
                <a href="#" class="btn">ðŸ“„ Paper</a>
                <a href="https://github.com/lhaippp/CamFlow" class="btn">ðŸ’» Code</a>
                <a href="#motivation" class="btn">ðŸ’¡ Motivation</a>
                <a href="#" class="btn">ðŸŽ¥ Video</a>
                <a href="#" class="btn">ï¿½ Data</a>
            </div>
        </div>
    </header>

    <main>
        <div class="container">
            <section id="abstract">
                <h2>Abstract</h2>
                <div class="abstract-box">
                    <p>
                        Estimating 2D camera motion is a fundamental computer vision task that models the projection of 3D camera movements onto the 2D image plane. 
                        Current methods rely on either homography-based approaches, limited to planar scenes, or meshflow techniques that use grid-based local homographies but struggle with complex non-linear transformations. 
                        <strong style="color: #667eea;">A key insight of our work is that combining flow fields from different homographies creates motion patterns that cannot be represented by any single homography.</strong>
                        We introduce <strong>CamFlow</strong>, a novel framework that represents camera motion using hybrid motion bases: physical bases derived from camera geometry and stochastic bases for complex scenarios. 
                        Our approach includes a hybrid probabilistic loss function based on the Laplace distribution that enhances training robustness. 
                        For evaluation, we create a new benchmark by masking dynamic objects in existing optical flow datasets to isolate pure camera motion. 
                        Experiments show CamFlow outperforms state-of-the-art methods across diverse scenarios, demonstrating superior robustness and generalization in zero-shot settings. 
                        Code and datasets are available at our project page: <a href="https://lhaippp.github.io/CamFlow/" target="_blank" style="color: #667eea; text-decoration: none;">https://lhaippp.github.io/CamFlow/</a>.
                    </p>
                </div>
            </section>

            <section id="motivation">
                <h2>Motivation</h2>
                <div class="content-box">
                    <div style="text-align: center; margin-bottom: 30px;">
                        <img src="images/motivation-hd.png" alt="Non-linearity of flow addition" style="max-width: 100%; height: auto; border-radius: 10px; box-shadow: 0 5px 15px rgba(0,0,0,0.1);">
                    </div>
                    <h3 style="color: #667eea; margin-bottom: 20px; text-align: center;">Non-linearity of Flow Addition</h3>
                    <p style="text-align: justify; line-height: 1.8; font-size: 1.05rem;">
                        <strong>Key Insight:</strong> Two homography matrices generate flow1 and flow2. Adding these flows (flow3) differs 
                        from the flow derived by multiplying the original homography matrices (homo3). When sampling points from flow3 to solve for a 
                        homography, we get inconsistent solutions, proving that <strong>combined flow fields cannot be represented by a single homography</strong>.
                    </p>
                    <p style="text-align: justify; line-height: 1.8; font-size: 1.05rem; margin-top: 20px;">
                        This fundamental limitation of traditional homography-based methods motivates our <strong>hybrid motion basis approach</strong>, 
                        which combines physical bases derived from camera geometry with stochastic bases to handle complex non-linear transformations 
                        that cannot be captured by any single homography.
                    </p>
                    <div class="key-insight">
                        <h4>
                            ðŸ’¡ <em>More bases â†’ Better camera motion representation</em>
                        </h4>
                    </div>
                </div>
            </section>

            <section id="method">
                <h2>Method Overview</h2>
                <div class="method-overview">
                    <p>Based on the insight that <strong>combined flow fields cannot be represented by a single homography</strong>, 
                    our <strong>CamFlow</strong> framework introduces hybrid motion bases for robust 2D camera motion estimation:</p>
                    
                    <div class="method-figure">
                        <img src="images/method-framework.png" alt="CamFlow Method Framework" style="max-width: 100%; height: auto; border-radius: 10px; box-shadow: 0 8px 25px rgba(0,0,0,0.15); margin: 30px 0;">
                        <div class="figure-caption">
                            <strong>Figure:</strong> Our proposed motion estimation framework. Given image pair <em>(I<sub>a</sub>, I<sub>b</sub>)</em>, features are extracted through a multi-scale pyramid and processed by the motion estimation transformer (MET) to compute weights for physical (blue) and noisy (red) motion bases. These weights linearly combine predefined motion bases to generate flow maps for warping. A mask generator predicts uncertainty masks <strong>d<sub>ab</sub></strong> and <strong>d<sub>ba</sub></strong> to reject unreliable regions, enhancing estimation robustness.
                        </div>
                    </div>
                    
                    <ul style="text-align: left; max-width: 700px; margin: 30px auto 0;">
                        <li><strong>Physical Motion Bases:</strong> Derived from camera geometry principles for fundamental motion patterns</li>
                        <li><strong>Stochastic Motion Bases:</strong> Learned representations for complex non-linear transformations that cannot be captured by single homographies</li>
                        <li><strong>Motion Estimation Transformer (MET):</strong> Computes adaptive weights for combining different motion bases</li>
                        <li><strong>Uncertainty Prediction:</strong> Mask generator rejects unreliable regions for enhanced robustness</li>
                        <li><strong>Hybrid Probabilistic Loss:</strong> Laplace distribution-based loss function for enhanced training robustness</li>
                    </ul>
                </div>
            </section>

            <section id="results">
                <h2>Results</h2>
                <div class="results-grid">
                    <div class="result-card">
                        <h3>Quantitative Results</h3>
                        <div class="result-placeholder">
                            Performance Comparison Table
                        </div>
                        <p>Our method achieves state-of-the-art performance on standard benchmarks.</p>
                    </div>
                    <div class="result-card">
                        <h3>Qualitative Results</h3>
                        <div class="result-placeholder">
                            Visual Comparison Results
                        </div>
                        <p>Visual comparison showing improved motion estimation accuracy.</p>
                    </div>
                    <div class="result-card">
                        <h3>Ablation Study</h3>
                        <div class="result-placeholder">
                            Ablation Study Results
                        </div>
                        <p>Analysis of individual component contributions to the overall performance.</p>
                    </div>
                </div>
            </section>

            <section id="citation">
                <h2>Citation</h2>
                <div class="citation-box">
@inproceedings{li2025estimating,
    title={Estimating 2D Camera Motion with Hybrid Motion Basis},
    author={Li, Haipeng and Zhou, Tianhao and Yang, Zhanglei and Wu, Yi and Chen, Yan and Mao, Zijing and Cheng, Shen and Zeng, Bing and Liu, Shuaicheng},
    booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
    pages={xxxx--xxxx},
    year={2025}
}
                </div>
            </section>
        </div>
    </main>

    <footer>
        <div class="container">
            <div style="display: flex; align-items: center; justify-content: center; gap: 15px; margin-bottom: 20px;">
                <img src="images/iccv2025-logo.png" alt="ICCV 2025" style="height: 40px; opacity: 0.8; border-radius: 5px;">
                <p style="margin: 0;">&copy; 2025 - Estimating 2D Camera Motion with Hybrid Motion Basis</p>
            </div>
            <div class="social-links">
                <a href="https://github.com/lhaippp/CamFlow">GitHub</a>
                <a href="#">arXiv</a>
                <a href="https://lhaippp.github.io/CamFlow/">Project Page</a>
            </div>
        </div>
    </footer>
    <script src="script.js"></script>
</body>
</html>
